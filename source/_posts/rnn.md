title: 循环神经网络（Recurrent Neural Network）
description: 
date: 2018-12-28 21:40:59
categories:
tags:
---

## 简介

在自然语言处理（NLP）中，需要处理的数据通常都是不定长的。例如，我们要构建一个神经网络模型，将下面这两句话翻译成英文：
- 这一世诺言从不曾忘。
- 深度学习的概念源于人工神经网络的研究。

这两句话的长度是不一样的，一般的神经网络输入的特征纬度是固定的，显然不能很好的解决这个问题，于是便出现了循环神经网络（Recurrent Neural Network，RNN）。

## 模型

### 基本的循环神经网络

一个最基本的循环神经网络由输入层，隐藏层和输出层构成，如下图所示：

![rnn-1](/resource/images/rnn-1.jpg)

<!-- more -->

这里的x、s、o分别是输入层、隐藏层和输出层，它们都是一个向量；W、U、V是连接层与层之间的权重矩阵。
RNN和一般的神经网络最大不同在于：
> RNN多了一个从隐藏层到隐藏层($s => s$)的过程，使RNN拥有了“记忆”的功能。
> (注意：这里的s要把他它看层多个隐藏层的多个神经单元，s是隐藏层单元构成的向量)

在RNN网络中，我们需要引入一个时间（顺序）的概念，我们把上图展开，RNN可以画成这样：

![rnn-2](/resource/images/rnn-2.jpg)

从图中可以看到，t 时刻的RNN网络输入值是 $x_t$，输出值是 $o_t$，隐藏层的值是 $s_t$，它的值取决于输入值 $x_t$ 和 t-1 时刻的隐藏层的值 $s_{t-1}$。

### 前向计算

RNN的每时间步的计算过程如下：
$$
\begin{aligned}
s_t =& g(Ux_t + Ws_{t-1}) \\\\
o_t =& f(Vs_t)
\end{aligned}
$$

其中，g、f 是激活函数，隐藏层 s 的初始值 $s_0$ 为零向量。

可以看出，RNN网络最后输出的结果受到所有输入序列 $x_1, x_2 ... x_T$ 的影响。因为隐藏 $s_{t-1}$ 保存了前面 t-1 个 x 值的结果，隐藏层 s 充当了一个“记忆”的角色。

### 优化目标
同样是求使得损失函数最小的权重 U 、 W 、V ；损失函数的形式根据具体的任务会有所不同。

### 梯度计算

这里RNN使用到的计算梯度的算法是BPTT（Back Propagation Trough Time），加上了时间的概念，是一种基于时间的反向传播算法。

虽然名字听上去很高大上的样子，但其实并不复杂，和普通的反向传播算法也差不多，把RNN展开之后，一样可以使用链式求导，这其实就很简单了。

## 梯度爆炸和梯度消失

## 代码实现

## 参考资料
