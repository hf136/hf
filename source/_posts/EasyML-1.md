title: 机器学习入门——什么是机器学习？
date: 2018-09-19 23:09:46
description: "有人说，现在的人工智能就是 $ y=ax+b $ ，虽然可能是开玩笑，但其实我觉得这句话很有道理。"
<!-- top: true -->
categories: Machine Learning
tags:
  - Machine Learning
  - Deep Learning
---

## 什么是机器学习？

有人说，现在的人工智能就是 $ y=ax+b $ ，虽然可能只是开玩笑，但其实我觉得这句话很有道理。

我们先来看一个经典的房价预测问题。为了将问题变得更加简单，我们只考虑一种因素：房子面积。下表给出了一组（构造的）数据，房子面积（平方米）和对应的房子价格（万元）。

![data](/resource/images/lr-1.png)

我们目的是构建一个模型，根据房子面积来预测出房子价格。只要你输入房子的面积，系统就能预测出房子的价格。我们从图中可以看出，房子的价格和房子面积具有线性关系，我们可以人为的画出一条直线，如下图所示，使用这条直线来作为预测房价的模型。而这条直线可以使用 $ y=ax+b $ 来表示，只要我们知道了a和b的值，通过输入房子面积$ x $，就能预测出房价价格 $ y $。

> 对于这个问题来说，所谓的机器学习就是：通过已有的房价数据，机器可以自动计算（学习）得到a和b，这样这个问题就解决了。

在机器学习没有出现之前，我们是可以通过一些人为方式来计算出a和b的。比如我先画出下图的直线，然后使用量角器量出直线与x轴的夹角，就能得到斜率从而计算出a，然后在直线随便找一个点(x1, y1)带入到式子中，就可以计算出b；或者直接带入两个点（x1, y1）、（x2, y2），通过解方程的方式计算出 a 和 b。

![line](/resource/images/lr-2.png)

## 如何自动学习得到a和b？

一般的做法是：我们可以先随机得到一个a和b的值，比如a=1，b=0，然后在不断的去调整a和b的值，最终得到最优的 a 和 b。
为了能够使计算机能够自动学习得到 a 和 b，我们需要一个衡量指标，那就是损失函数（loss function）或者称作代价函数（cost function），它的作用的衡量模型的好坏。

### 损失函数
在这个问题中，我们可以使用平均平方误差来当做我们的损失函数，即：我们有 m 条训练数据，我们可以通过公式 $ loss = \frac {1} {m} \sum_{i=0}^m (y_i - y_i^\prime)^2 $ 来计算模型预测的值 $y^\prime$ 和真实值 $y$ 之间的平方误差。

当模型预测的准确率是100%的时候，损失函数的值等于0，也就是说损失函数的结果越小，模型的效果越好。有了这个损失函数，我们就可以知道哪些 a 和 b 的值是比较好的，这样机器就可以知道哪些 a 和 b 的值是比较好的了。

我们很容易想到：我们可以将所有可能的 a 和 b 的值遍历一遍，通过已有的数据去计算损失函数的值，取损失函数最小时所对应的 a 和 b 的值，那问题不就解决了吗？

是的，但是这种方法太笨了，a 和 b 的值是有无限多可能的，而且计算量太大，所以我们还需要一种有效的学习方法：**梯度下降**。

### 梯度下降

什么是梯度下降（Gradient Descent）呢？

在这个问题中，我们的损失函数实际上是一个二次函数，我们可以简单的理解为 a 或 b 和对应损失函数的值之间的关系如下图所示：

![loss](/resource/images/lr-loss.png)

在上图中，横坐标表示 a 的取值，纵坐标表示 loss 的值。我们可以知道当 a=2.3 时, 损失函数 loss 的值最小，那如果一开始我们随机得到的 a 的初始值是1.7或者3.6的话，怎么样才能快速的得到一个接近最优值2.3的值呢？

> 答案是：导数（只有一个变量是导数，在多变量中为偏导数）。
>
> 我们可以发现：当 a > 2.3 时，比如说 a = 3.6，此时 a 的导数（切线的斜率）大于0；当 a < 2.3 时，比如说 a = 1.7，此时 a 的导数小于0；而当 a = 2.3 时，a 的导数等于0。

假设一开我们随机得到: a = 3.6，那么 a 需要向左移动，即计算 a 的导数 $ \nabla_a	$，就可以知道 a 需要调整移动的方向。我们可以通过以下公式得到新的 a 值 $a_{new}$ 为：$$ a_{new}=a_{old} - \alpha\nabla_a $$

其中 $\alpha$ 是一个大于 0 的系数，通常称作**学习率**，控制着 a 每一次调整的步长。如下图所示，a 每次的调整如果过大的话，很容易调整过头；如果每次的调整过小的话，学习的速度就会非常慢。所以我们需要一个学习率去调整控制学习的速度。 

![loss](/resource/images/lr-loss2.png)

如上图所示，经过多次迭代，我们就可以快速得到一个无限接近于最优值的 $ a_{best}$ ，同理，b 的最优值 $b_{best}$ 也可以通过这种方式得到。$$ b_{new}=b_{old} - \alpha\nabla_b $$

梯度下降中，**梯度**指的是 a 和 b 的导数（实际上是偏导数）组成的向量：$(\nabla_a, \nabla_b)$ ；**下降**是指损失函数值 loss 不断减小的过程。

## 总结

在这里，机器学习就是:
> 在已有的数据（训练数据）的基础上，先建立一个数学模型 $y=ax+b$，再定义一个损失函数 $loss$，最后通过梯度下降的方式不断的调整模型参数 a 和 b，使损失函数的值不断变小，得到最优的参数 $ a_{best}$ 和 $ b_{best}$ 的过程。

推广到一般的情况（考虑多种影响房价的因素），这里的 $x$ 变成了一组值 $x_1, x_2, ... , x_n$, a也相应的变成了一组值，为了更加形象一些，一般我们使用 weight（权重）的首字母 $w$ 来代替 a ，即：$w_1, w_2, ... , w_n$，这里的线性回归模型就变成了：$$y=w_1 x_1 + w_2 x_2 + ... w_n x_n + b$$

同样，我们可以通过计算每一个 $w_i$ 的偏导数，使用梯度下降的学习方法来不断迭代更新得到最优的 $w$ 值。我们可以将上面的公式简写成向量内积的形式： $ y=\vec{w} \cdot \vec{x} + b $

## 再谈谈深度学习

深度学习可谓是人工智能领域中最“智能”的分支，而深度学习中无论多么复杂的模型都离不开 $ y=\vec{w} \cdot \vec{x} + b $ ，因为它是“神经元”的重要组成部分。

神经元一般由 $ y=\vec{w} \cdot \vec{x} + b $ （线性部分）加上一个激活函数（非线性部分）组成。

在深度学习中，无论多么复杂的模型几乎都离不开**神经元**，神经元以不同的“空间结构”组合在一起，构成了各种各样的复杂模型。
