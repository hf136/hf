<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>wyq&#39;s blog</title>
  
  <subtitle>仰望星空，寻找属于自己的路</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://hf136.github.io/"/>
  <updated>2018-09-27T16:03:42.257Z</updated>
  <id>http://hf136.github.io/</id>
  
  <author>
    <name>wyq</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>线性回归</title>
    <link href="http://hf136.github.io/2018/09/27/linear-regression/"/>
    <id>http://hf136.github.io/2018/09/27/linear-regression/</id>
    <published>2018-09-27T14:30:33.000Z</published>
    <updated>2018-09-27T16:03:42.257Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>$$y={\bf{wx}} +b$$</p><p>其中，${\bf{w}}$ 和 ${\bf{x}}$ 都是向量， ${\bf{w}} = w_1, w_2, …, w_n$ 表示要学习的模型参数, ${\bf{x}} = x_1, x_2, …, x_n$ 表示模型的输入。</p><h2 id="损失函数（代价函数）"><a href="#损失函数（代价函数）" class="headerlink" title="损失函数（代价函数）"></a>损失函数（代价函数）</h2><p>$$L({\bf{w}}, b) = \frac{1}{2m} \sum_{i=1}^{m}{(y^\prime_i - y_i)^2}$$</p><p>其中，$m$ 表示训练样本数， $y^\prime$ 表示模型输出结果， $y$ 表示实际结果。</p><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>求使得损失函数最小的参数 ${\bf{w}}$ 和 $b$ 。</p><p>$$\min_{w,b}L({\bf{w}}, b)$$</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>对每个 $w_j$ 和 $b$ 求偏导数，并不断迭代：</p><p>$$w_j := w_j - \alpha \frac{1}{m} \sum_{i=1}^{m}{(y^\prime_i - y_i)x_j}$$</p><p>$$b := b - \alpha \frac{1}{m} \sum_{i=1}^{m}{(y^\prime_i - y_i)}$$</p><p>其中， $\alpha$ 为学习速率， $\alpha$ 后面的项为偏导数。</p>]]></content>
    
    <summary type="html">
    
       
    
    </summary>
    
      <category term="Machine Learning" scheme="http://hf136.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://hf136.github.io/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://hf136.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>函数的导数</title>
    <link href="http://hf136.github.io/2018/09/20/math/"/>
    <id>http://hf136.github.io/2018/09/20/math/</id>
    <published>2018-09-20T13:09:46.000Z</published>
    <updated>2018-09-27T14:11:53.592Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常见基本函数导数"><a href="#常见基本函数导数" class="headerlink" title="常见基本函数导数"></a>常见基本函数导数</h2><table><thead><tr><th>导数名</th><th>原函数</th><th>导函数</th></tr></thead><tbody><tr><td>常函数（常数）</td><td>$y=C$ (C为常数)</td><td>$y^\prime=0$</td></tr><tr><td>幂函数</td><td>$y=x^n$</td><td>$y^\prime=nx^{n-1}$</td></tr><tr><td>指数函数</td><td>$y=a^x$</td><td>$y^\prime=a^x\ln x$</td></tr><tr><td></td><td>$y=e^x$</td><td>$y^\prime=e^x$</td></tr><tr><td>对数函数</td><td>$y=\log_a x$</td><td>$y^\prime=\frac{1}{x\ln a}$</td></tr><tr><td></td><td>$y=\ln x$</td><td>$y^\prime=\frac{1}{x}$</td></tr><tr><td>正弦函数</td><td>$y=\sin x$</td><td>$y^\prime=\cos x$</td></tr><tr><td>余弦函数</td><td>$y=\cos x$</td><td>$y^\prime=-\sin x$</td></tr></tbody></table><h2 id="复合函数求导"><a href="#复合函数求导" class="headerlink" title="复合函数求导"></a>复合函数求导</h2><p>原函数：$y^\prime=f(g(x))$， 其中 $y=f(u)$， $u=g(x)$</p><p>使用链式法则求导：$y^\prime = f^\prime(u)u^\prime(x) = f^\prime(g(x))g^\prime(x)$</p><h2 id="导数的四则运算"><a href="#导数的四则运算" class="headerlink" title="导数的四则运算"></a>导数的四则运算</h2><p>$$(u \pm v)^\prime = u^\prime \pm v^\prime$$</p><p>$$(uv)^\prime = u^\prime v + u v^\prime$$</p><p>$$(\frac{u}{v})^\prime = \frac{u^\prime v - u v^\prime}{v^2}$$</p>]]></content>
    
    <summary type="html">
    
      整理一些常见的函数的导数
    
    </summary>
    
      <category term="Math" scheme="http://hf136.github.io/categories/Math/"/>
    
    
      <category term="Math" scheme="http://hf136.github.io/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门——什么是机器学习？</title>
    <link href="http://hf136.github.io/2018/09/19/EasyML-1/"/>
    <id>http://hf136.github.io/2018/09/19/EasyML-1/</id>
    <published>2018-09-19T15:09:46.000Z</published>
    <updated>2018-09-27T16:24:16.592Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h2><p>有人说，现在的人工智能就是 $ y=ax+b $ ，虽然可能只是开玩笑，但其实我觉得这句话很有道理。</p><p>我们先来看一个经典的房价预测问题。为了将问题变得更加简单，我们只考虑一种因素：房子面积。下表给出了一组（构造的）数据，房子面积（平方米）和对应的房子价格（万元）。</p><p><img src="/resource/images/lr-1.png" alt="data"></p><p>我们目的是构建一个模型，根据房子面积来预测出房子价格。只要你输入房子的面积，系统就能预测出房子的价格。我们从图中可以看出，房子的价格和房子面积具有线性关系，我们可以人为的画出一条直线，如下图所示，使用这条直线来作为预测房价的模型。而这条直线可以使用 $ y=ax+b $ 来表示，只要我们知道了a和b的值，通过输入房子面积$ x $，就能预测出房价价格 $ y $。</p><blockquote><p>对于这个问题来说，所谓的机器学习就是：通过已有的房价数据，机器可以自动计算（学习）得到a和b，这样这个问题就解决了。</p></blockquote><p>在机器学习没有出现之前，我们是可以通过一些人为方式来计算出a和b的。比如我先画出下图的直线，然后使用量角器量出直线与x轴的夹角，就能得到斜率从而计算出a，然后在直线随便找一个点(x1, y1)带入到式子中，就可以计算出b；或者直接带入两个点（x1, y1）、（x2, y2），通过解方程的方式计算出 a 和 b。</p><p><img src="/resource/images/lr-2.png" alt="line"></p><h2 id="如何自动学习得到a和b？"><a href="#如何自动学习得到a和b？" class="headerlink" title="如何自动学习得到a和b？"></a>如何自动学习得到a和b？</h2><p>一般的做法是：我们可以先随机得到一个a和b的值，比如a=1，b=0，然后在不断的去调整a和b的值，最终得到最优的 a 和 b。<br>为了能够使计算机能够自动学习得到 a 和 b，我们需要一个衡量指标，那就是损失函数（loss function）或者称作代价函数（cost function），它的作用的衡量模型的好坏。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在这个问题中，我们可以使用平均平方误差来当做我们的损失函数，即：我们有 m 条训练数据，我们可以通过公式 $ loss = \frac {1} {m} \sum_{i=0}^m (y_i - y_i^\prime)^2 $ 来计算模型预测的值 $y^\prime$ 和真实值 $y$ 之间的平方误差。</p><p>当模型预测的准确率是100%的时候，损失函数的值等于0，也就是说损失函数的结果越小，模型的效果越好。有了这个损失函数，我们就可以知道哪些 a 和 b 的值是比较好的，这样机器就可以知道哪些 a 和 b 的值是比较好的了。</p><p>我们很容易想到：我们可以将所有可能的 a 和 b 的值遍历一遍，通过已有的数据去计算损失函数的值，取损失函数最小时所对应的 a 和 b 的值，那问题不就解决了吗？</p><p>是的，但是这种方法太笨了，a 和 b 的值是有无限多可能的，而且计算量太大，所以我们还需要一种有效的学习方法：<strong>梯度下降</strong>。</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>什么是梯度下降（Gradient Descent）呢？</p><p>在这个问题中，我们的损失函数实际上是一个二次函数，我们可以简单的理解为 a 或 b 和对应损失函数的值之间的关系如下图所示：</p><p><img src="/resource/images/lr-loss.png" alt="loss"></p><p>在上图中，横坐标表示 a 的取值，纵坐标表示 loss 的值。我们可以知道当 a=2.3 时, 损失函数 loss 的值最小，那如果一开始我们随机得到的 a 的初始值是1.7或者3.6的话，怎么样才能快速的得到一个接近最优值2.3的值呢？</p><blockquote><p>答案是：导数（只有一个变量是导数，在多变量中为偏导数）。</p><p>我们可以发现：当 a &gt; 2.3 时，比如说 a = 3.6，此时 a 的导数（切线的斜率）大于0；当 a &lt; 2.3 时，比如说 a = 1.7，此时 a 的导数小于0；而当 a = 2.3 时，a 的导数等于0。</p></blockquote><p>假设一开我们随机得到: a = 3.6，那么 a 需要向左移动，即计算 a 的导数 $ \nabla_a    $，就可以知道 a 需要调整移动的方向。我们可以通过以下公式得到新的 a 值 $a_{new}$ 为：$$ a_{new}=a_{old} - \alpha\nabla_a $$</p><p>其中 $\alpha$ 是一个大于 0 的系数，通常称作<strong>学习率</strong>，控制着 a 每一次调整的步长。如下图所示，a 每次的调整如果过大的话，很容易调整过头；如果每次的调整过小的话，学习的速度就会非常慢。所以我们需要一个学习率去调整控制学习的速度。<br><br><img src="/resource/images/lr-loss2.png" alt="loss"></p><p>如上图所示，经过多次迭代，我们就可以快速得到一个无限接近于最优值的 $ a_{best}$ ，同理，b 的最优值 $b_{best}$ 也可以通过这种方式得到。$$ b_{new}=b_{old} - \alpha\nabla_b $$</p><p>梯度下降中，<strong>梯度</strong>指的是 a 和 b 的导数（实际上是偏导数）组成的向量：$(\nabla_a, \nabla_b)$ ；<strong>下降</strong>是指损失函数值 loss 不断减小的过程。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这里，机器学习就是:</p><blockquote><p>在已有的数据（训练数据）的基础上，先建立一个数学模型 $y=ax+b$，再定义一个损失函数 $loss$，最后通过梯度下降的方式不断的调整模型参数 a 和 b，使损失函数的值不断变小，得到最优的参数 $ a_{best}$ 和 $ b_{best}$ 的过程。</p></blockquote><p>推广到一般的情况（考虑多种影响房价的因素），这里的 $x$ 变成了一组值 $x_1, x_2, … , x_n$, a也相应的变成了一组值，为了更加形象一些，一般我们使用 weight（权重）的首字母 $w$ 来代替 a ，即：$w_1, w_2, … , w_n$，这里的线性回归模型就变成了：$$y=w_1 x_1 + w_2 x_2 + … w_n x_n + b$$</p><p>同样，我们可以通过计算每一个 $w_i$ 的偏导数，使用梯度下降的学习方法来不断迭代更新得到最优的 $w$ 值。我们可以将上面的公式简写成向量内积的形式： $ y=\vec{w} \cdot \vec{x} + b $</p><h2 id="再谈谈深度学习"><a href="#再谈谈深度学习" class="headerlink" title="再谈谈深度学习"></a>再谈谈深度学习</h2><p>深度学习可谓是人工智能领域中最“智能”的分支，而深度学习中无论多么复杂的模型都离不开 $ y=\vec{w} \cdot \vec{x} + b $ ，因为它是“神经元”的重要组成部分。</p><p>神经元一般由 $ y=\vec{w} \cdot \vec{x} + b $ （线性部分）加上一个激活函数（非线性部分）组成。</p><p>在深度学习中，无论多么复杂的模型几乎都离不开<strong>神经元</strong>，神经元以不同的“空间结构”组合在一起，构成了各种各样的复杂模型。</p>]]></content>
    
    <summary type="html">
    
      有人说，现在的人工智能就是 $ y=ax+b $ ，虽然可能是开玩笑，但其实我觉得这句话很有道理。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://hf136.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://hf136.github.io/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://hf136.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
